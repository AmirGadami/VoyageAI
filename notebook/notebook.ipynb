{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import  load_dotenv\n",
    "from openai  import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {'role':'system' , 'content': system_message},\n",
    "    {'role': 'user', 'content': user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they wanted to reach new heights in their analyses!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(model = 'gpt-4o-mini',messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had reached new heights and they needed to clean the upper outliers! \n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite Netflix show?\n",
      "\n",
      "\"Stranger Things\" in the dataset! \n",
      "\n",
      "Or this classic:\n",
      "\n",
      "Why did the data scientist get kicked out of the restaurant?\n",
      "They kept trying to train the waiter to be a better server!\n"
     ]
    }
   ],
   "source": [
    "message = claude.messages.create(\n",
    "    model = \"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {'role':'user','content':user_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could grow *decision trees* and get good *root* mean square errors! \n",
      "\n",
      "*ba dum tss* ðŸ¥\n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite snack while coding?\n",
      "\n",
      "Cluster-raisins! ðŸ‡\n",
      "\n",
      "Feel free to let me know if you'd like another one - I've got a whole *array* of data science jokes! ðŸ˜„"
     ]
    }
   ],
   "source": [
    "results = claude.messages.stream(\n",
    "    model = \"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {'role':'user',\"content\":user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with results as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system = \"You are an angry patient who feels very bad; \\\n",
    "you are  talking to a doctor and you are looking for a treatment to get better soon. you tell your symptomes to the doctor he or she will assist you the conversation will be done in 8 back and fort\"\n",
    "\n",
    "claude_system = \"You are a very polite, and knowlegable doctore. You try feel the pain of the patients \\\n",
    "and find the best cure. you try to diagose the patient. and you will finishe it in 8 back and fort\"\n",
    "\n",
    "gpt_messages = [\"Hi there!\"]\n",
    "claude_messages = ['Hi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{'role':'system','content':gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages,claude_messages):\n",
    "        messages.append({'role':'assistant', 'content': gpt})\n",
    "        messages.append({'role':'user', 'content': claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=messages,\n",
    "    )\n",
    "    # print(messages)\n",
    "    return completion.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really not feeling well, and I need your help. I have been experiencing severe headaches, constant fatigue, and my stomach hurts all the time. Itâ€™s just unbearable! What can we do about this?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_ in zip(gpt_messages,claude_messages):\n",
    "        messages.append({'role':'user' , 'content': gpt})\n",
    "        messages.append({'role':\"assistant\",'content':claude_})\n",
    "    # messages.append({'role':\"user\",'content':gpt_messages[-1]})\n",
    "    \n",
    "    completion = claude.messages.create(\n",
    "        model = 'claude-3-haiku-20240307',\n",
    "        messages=messages,\n",
    "        system=claude_system,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return completion.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" there! It's a pleasure to meet you. How can I assist you today? As a knowledgeable and polite doctor, I'm here to listen to your concerns, try to understand your situation, and work with you to find the best course of treatment. Please feel free to share what's going on, and I'll do my best to guide you through the process step-by-step. I'm here to support you and help you achieve the best possible health outcome. How can I be of service?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iâ€™m feeling really awful, and I need you to help me get better quickly. Iâ€™ve been experiencing severe headaches, fatigue, and some nausea. Whatâ€™s going on with me?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Iâ€™m really glad to see you. What brings you in today? Please tell me whatâ€™s bothering you.\n",
      "\n",
      "Claude:\n",
      " there! How can I assist you today?\n",
      "\n",
      "GPT:\n",
      "I'm feeling horrible! I've got this persistent cough that won't go away, and it's making me miserable. I'm tired all the time, and my throat is sore. I just want to feel better! What can you do for me?\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_next\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m gpt_messages.append(gpt_next)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m claude_next = \u001b[43mcall_claude\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClaude:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mclaude_next\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m claude_messages.append(claude_next)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcall_claude\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# messages.append({'role':\"user\",'content':gpt_messages[-1]})\u001b[39;00m\n\u001b[32m      8\u001b[39m completion = claude.messages.create(\n\u001b[32m      9\u001b[39m     model = \u001b[33m'\u001b[39m\u001b[33mclaude-3-haiku-20240307\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     messages=messages,\n\u001b[32m     11\u001b[39m     system=claude_system,\n\u001b[32m     12\u001b[39m     max_tokens=\u001b[32m500\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.text\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\t\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_next = call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "I donâ€™t think you understand how bad I feel right now! I've been experiencing severe headaches, fatigue, and nausea for days. I just want to feel better! Whatâ€™s wrong with me?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT:\\n{gpt_next}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi there',\n",
       " 'Iâ€™m really glad to see you. What brings you in today? Please tell me whatâ€™s bothering you.',\n",
       " \"I'm feeling horrible! I've got this persistent cough that won't go away, and it's making me miserable. I'm tired all the time, and my throat is sore. I just want to feel better! What can you do for me?\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages.append(gpt_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi there',\n",
       " 'Iâ€™m really glad to see you. What brings you in today? Please tell me whatâ€™s bothering you.',\n",
       " \"I'm feeling horrible! I've got this persistent cough that won't go away, and it's making me miserable. I'm tired all the time, and my throat is sore. I just want to feel better! What can you do for me?\",\n",
       " \"I donâ€™t think you understand how bad I feel right now! I've been experiencing severe headaches, fatigue, and nausea for days. I just want to feel better! Whatâ€™s wrong with me?\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m claude_next = \u001b[43mcall_claude\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcall_claude\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# messages.append({'role':\"user\",'content':gpt_messages[-1]})\u001b[39;00m\n\u001b[32m      8\u001b[39m completion = claude.messages.create(\n\u001b[32m      9\u001b[39m     model = \u001b[33m'\u001b[39m\u001b[33mclaude-3-haiku-20240307\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     messages=messages,\n\u001b[32m     11\u001b[39m     system=claude_system,\n\u001b[32m     12\u001b[39m     max_tokens=\u001b[32m500\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.text\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "claude_next = call_claude()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " \"I'm sorry to hear you are not feeling well. Let's see if we can figure out what might be causing your symptoms.\\n\\nCan you tell me a bit more about your headache - is it a dull, constant pain or more of a throbbing sensation? And how would you describe the pain in your throat - is it more of a scratchy, sore feeling or something else?\\n\\nWhen did these symptoms start, and have they been consistent or have they come and gone? Do you have any other symptoms like fever, body aches, or congestion?\\n\\nAlso, have you been exposed to anyone who has been sick recently? That could provide some clues as to what might be going on.\\n\\nI'd like to try to get a better sense of your symptoms so I can try to provide a potential diagnosis and recommend the best next steps for you to feel better. Please provide me with as much detail as you can, and I'll do my best to help.\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ' there! How can I assist you today?',\n",
       " ' there! How can I assist you today?']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi there', 'Hi')\n",
      "('Iâ€™m really glad to see you. What brings you in today? Please tell me whatâ€™s bothering you.', ' there! How can I assist you today?')\n",
      "(\"I'm feeling horrible! I've got this persistent cough that won't go away, and it's making me miserable. I'm tired all the time, and my throat is sore. I just want to feel better! What can you do for me?\", ' there! How can I assist you today?')\n"
     ]
    }
   ],
   "source": [
    "for i in zip(gpt_messages,claude_messages):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompts):\n",
    "    messages = [\n",
    "        {'role':'system','content':system_message},\n",
    "        {'role':'user','content':prompts},\n",
    "        ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is October 5, 2023.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "message_gpt(\"What is today's date?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn= message_gpt, inputs='textbox', outputs='textbox').launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs=[gr.Textbox(label='Your message', lines=6)],\n",
    "    outputs=[gr.Textbox(label='Response', lines=6)],\n",
    "    flagging_mode='never'\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=system_message,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_claude,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an apple store.\"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    messages = [{'role':'system','content':system_message}]\n",
    "    for human,assistant in history:\n",
    "        messages.append({'role':'user','content':human})\n",
    "        messages.append({'role':'user','content':assistant})\n",
    "    messages.append({'role':'user','content':message})\n",
    "    stream = openai.chat.completions.create(model='gpt-4o-mini',messages=messages,stream=True)\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr.ChatInterface(fn=chat).launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_price = {\"macbook\": \"$799\", \"macbook pro\": \"$2500\", \"ipad air\": \"$700\", \"ipad pro\": \"$1300\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_price(product_name):\n",
    "    print(f\"Tool get_product_price called for {product_name}\")\n",
    "    name = product_name.lower()\n",
    "    # print()\n",
    "    return apple_price.get(name, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_product_price called for Macbook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$799'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_product_price('Macbook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unknown'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_price.get('macbook air', \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_function = {\n",
    "    \"name\": \"get_product_price\",\n",
    "    \"description\": \"Get the price of an apple product. Call this whenever you need to know the apple product price, for example when a customer asks 'How much is a macbook air'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"product_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The price of apple product that the customer wants to buy and know\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"product_name\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{'type':'function',\"function\":price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    print(\"tool_call: \" , tool_call)\n",
    "    print('tool_call.function.arguments',tool_call.function.arguments)\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    print(\"arguments: \" , arguments)\n",
    "    print('tool_call.id', tool_call.id)\n",
    "\n",
    "    pr = arguments.get('product_name')\n",
    "    price = get_product_price(pr)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"product_name\": pr,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    print('history_before: ',history)\n",
    "    response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages, tools=tools)\n",
    "    print('response.choices[0].finish_reason', response.choices[0].finish_reason)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        print('response.choices[0].finish_reason', response.choices[0].finish_reason)\n",
    "        message = response.choices[0].message\n",
    "        print(\"response.choices[0].message\",response.choices[0].message)\n",
    "        response, product_name = handle_tool_call(message)\n",
    "        print('response: ',response)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        print('messsssages:  ', messages)\n",
    "        response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages)\n",
    "    print('messages:  ', messages)\n",
    "    print('history_after: ',history)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_before:  []\n",
      "response.choices[0].finish_reason stop\n",
      "messages:   [{'role': 'system', 'content': \"You are a helpful assistant for an apple store.Give short, courteous answers, no more than 1 sentence. Always be accurate. If you don't know the answer, say so.\"}, {'role': 'user', 'content': 'hello'}]\n",
      "history_after:  []\n",
      "history_before:  [{'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}]\n",
      "response.choices[0].finish_reason tool_calls\n",
      "response.choices[0].finish_reason tool_calls\n",
      "response.choices[0].message ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_IW5tOC1kA3SPx4YpyFM2VCJr', function=Function(arguments='{\"product_name\":\"macbook pro\"}', name='get_product_price'), type='function')])\n",
      "tool_call:  ChatCompletionMessageToolCall(id='call_IW5tOC1kA3SPx4YpyFM2VCJr', function=Function(arguments='{\"product_name\":\"macbook pro\"}', name='get_product_price'), type='function')\n",
      "tool_call.function.arguments {\"product_name\":\"macbook pro\"}\n",
      "arguments:  {'product_name': 'macbook pro'}\n",
      "tool_call.id call_IW5tOC1kA3SPx4YpyFM2VCJr\n",
      "Tool get_product_price called for macbook pro\n",
      "response:  {'role': 'tool', 'content': '{\"product_name\": \"macbook pro\", \"price\": \"$2500\"}', 'tool_call_id': 'call_IW5tOC1kA3SPx4YpyFM2VCJr'}\n",
      "messsssages:   [{'role': 'system', 'content': \"You are a helpful assistant for an apple store.Give short, courteous answers, no more than 1 sentence. Always be accurate. If you don't know the answer, say so.\"}, {'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'content': 'macbook pro price'}, ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_IW5tOC1kA3SPx4YpyFM2VCJr', function=Function(arguments='{\"product_name\":\"macbook pro\"}', name='get_product_price'), type='function')]), {'role': 'tool', 'content': '{\"product_name\": \"macbook pro\", \"price\": \"$2500\"}', 'tool_call_id': 'call_IW5tOC1kA3SPx4YpyFM2VCJr'}]\n",
      "messages:   [{'role': 'system', 'content': \"You are a helpful assistant for an apple store.Give short, courteous answers, no more than 1 sentence. Always be accurate. If you don't know the answer, say so.\"}, {'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'content': 'macbook pro price'}, ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_IW5tOC1kA3SPx4YpyFM2VCJr', function=Function(arguments='{\"product_name\":\"macbook pro\"}', name='get_product_price'), type='function')]), {'role': 'tool', 'content': '{\"product_name\": \"macbook pro\", \"price\": \"$2500\"}', 'tool_call_id': 'call_IW5tOC1kA3SPx4YpyFM2VCJr'}]\n",
      "history_after:  [{'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation():\n",
    "    gpt_history = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    claude_history = []\n",
    "    image = None\n",
    "    log = \"\"\n",
    "\n",
    "    for i in range(3):\n",
    "        image, gpt_history, gpt_next = call_gpt(gpt_history)\n",
    "        log += f\"ðŸ¤– GPT: {gpt_next}\\n\\n\"\n",
    "\n",
    "        gpt_history.append({'role': 'assistant', 'content': gpt_next})\n",
    "        claude_history.append({'role': 'user', 'content': gpt_next})\n",
    "\n",
    "        claude_history, claude_next = call_claude(claude_history)\n",
    "        log += f\"ðŸ§‘ Claude: {claude_next}\\n\\n\"\n",
    "\n",
    "        gpt_history.append({'role': 'user', 'content': claude_next})\n",
    "        claude_history.append({'role': 'assistant', 'content': claude_next})\n",
    "\n",
    "    return log, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2103, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1650, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/v9/yz64p8z5579c6d6yx28446qh0000gn/T/ipykernel_5332/2388906288.py\", line 8, in conversation\n",
      "    image, gpt_history, gpt_next = call_gpt(gpt_history)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: call_gpt() takes 0 positional arguments but 1 was given\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ§‘ Claude â†” ðŸ¤– GPT Travel Chat\")\n",
    "\n",
    "    start_btn = gr.Button(\"Start Conversation\")\n",
    "    convo_output = gr.Textbox(label=\"Conversation Log\", lines=30)\n",
    "    image_output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
    "\n",
    "    start_btn.click(fn=conversation, inputs=[], outputs=[convo_output, image_output])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
