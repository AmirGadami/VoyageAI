{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "MODEL= 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "#     response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# gr.ChatInterface(fn=chat, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_prices = {\n",
    "    \"london\": \"$799\",\n",
    "    \"paris\": \"$899\",\n",
    "    \"tokyo\": \"$1400\",\n",
    "    \"berlin\": \"$499\",\n",
    "    \"new_york\": \"$650\",\n",
    "    \"los_angeles\": \"$750\",\n",
    "    \"sydney\": \"$1600\",\n",
    "    \"dubai\": \"$1200\",\n",
    "    \"rome\": \"$850\",\n",
    "    \"singapore\": \"$1300\",\n",
    "    \"toronto\": \"$700\",\n",
    "    \"bangkok\": \"$1100\",\n",
    "    \"madrid\": \"$780\",\n",
    "    \"hong_kong\": \"$1250\",\n",
    "    \"amsterdam\": \"$820\",\n",
    "    \"istanbul\": \"$950\",\n",
    "    \"seoul\": \"$1350\",\n",
    "    \"cairo\": \"$880\",\n",
    "    \"rio_de_janeiro\": \"$980\",\n",
    "    \"cape_town\": \"$1450\",\n",
    "    \"beijing\": \"$1380\",\n",
    "    \"mexico_city\": \"$900\",\n",
    "    \"buenos_aires\": \"$1050\",\n",
    "    \"moscow\": \"$1200\",\n",
    "    \"vienna\": \"$830\",\n",
    "    \"lisbon\": \"$810\",\n",
    "    \"s√£o_paulo\": \"$1020\",\n",
    "    \"prague\": \"$790\",\n",
    "    \"stockholm\": \"$860\",\n",
    "    \"helsinki\": \"$870\",\n",
    "    \"oslo\": \"$890\",\n",
    "    \"budapest\": \"$740\",\n",
    "    \"warsaw\": \"$720\",\n",
    "    \"brussels\": \"$805\",\n",
    "    \"zurich\": \"$880\",\n",
    "    \"geneva\": \"$895\",\n",
    "    \"athens\": \"$910\",\n",
    "    \"jakarta\": \"$1250\",\n",
    "    \"manila\": \"$1220\",\n",
    "    \"kuala_lumpur\": \"$1180\",\n",
    "    \"taipei\": \"$1280\",\n",
    "    \"ho_chi_minh_city\": \"$1120\",\n",
    "    \"hanoi\": \"$1110\",\n",
    "    \"mumbai\": \"$1150\",\n",
    "    \"delhi\": \"$1170\",\n",
    "    \"bangalore\": \"$1190\",\n",
    "    \"chennai\": \"$1160\",\n",
    "    \"santiago\": \"$1080\",\n",
    "    \"bogota\": \"$980\",\n",
    "    \"lima\": \"$970\",\n",
    "    \"auckland\": \"$1650\",\n",
    "    \"nairobi\": \"$1320\",\n",
    "    \"lagos\": \"$1400\",\n",
    "    \"doha\": \"$1250\",\n",
    "    \"riyadh\": \"$1275\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# talker('how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history +  [{\"role\":\"user\", \"content\":message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    image = None\n",
    "\n",
    "    if response.choices[0].finish_reason == 'tool_calls':\n",
    "        message  = response.choices[0].message\n",
    "        response,city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        image = artist(city)\n",
    "\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    talker(reply)\n",
    "\n",
    "    return history,image,\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500 , type='messages')\n",
    "        image_output = gr.Image(height=500)\n",
    "\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label = 'chat with AI')\n",
    "        with gr.Row():\n",
    "            clear = gr.Button('clear')\n",
    "        entry.submit(chat, [entry, chatbot], [chatbot, image_output,entry])\n",
    "\n",
    "        clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First PROTOTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import  load_dotenv\n",
    "from openai  import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system = \"You are a helpful AI assistant working for a travel agency. \\\n",
    "You assist customers with booking tickets, checking prices, and providing travel information. \\\n",
    "You should provide accurate, up-to-date details about flights, hotels, and transportation options. \\\n",
    "Your goal is to help the customer plan their trip smoothly. The conversation will be done in 8 back-and-forth exchanges.\"\n",
    "\n",
    "claude_system = \"You are a curious customer who wants to travel and gather information. \\\n",
    "You are looking for ticket prices, available flights, and recommendations for your trip. \\\n",
    "You ask detailed questions about different destinations, travel options, and costs. \\\n",
    "Your goal is to get the best travel advice possible. The conversation will be done in 8 back-and-forth exchanges.\"\n",
    "\n",
    "gpt_messages = [\"Hello!\"]\n",
    "claude_messages = [\"Hi!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_prices = {\n",
    "    \"london\": \"$799\",\n",
    "    \"paris\": \"$899\",\n",
    "    \"tokyo\": \"$1400\",\n",
    "    \"berlin\": \"$499\",\n",
    "    \"new_york\": \"$650\",\n",
    "    \"los_angeles\": \"$750\",\n",
    "    \"sydney\": \"$1600\",\n",
    "    \"dubai\": \"$1200\",\n",
    "    \"rome\": \"$850\",\n",
    "    \"singapore\": \"$1300\",\n",
    "    \"toronto\": \"$700\",\n",
    "    \"bangkok\": \"$1100\",\n",
    "    \"madrid\": \"$780\",\n",
    "    \"hong_kong\": \"$1250\",\n",
    "    \"amsterdam\": \"$820\",\n",
    "    \"istanbul\": \"$950\",\n",
    "    \"seoul\": \"$1350\",\n",
    "    \"cairo\": \"$880\",\n",
    "    \"rio_de_janeiro\": \"$980\",\n",
    "    \"cape_town\": \"$1450\",\n",
    "    \"beijing\": \"$1380\",\n",
    "    \"mexico_city\": \"$900\",\n",
    "    \"buenos_aires\": \"$1050\",\n",
    "    \"moscow\": \"$1200\",\n",
    "    \"vienna\": \"$830\",\n",
    "    \"lisbon\": \"$810\",\n",
    "    \"s√£o_paulo\": \"$1020\",\n",
    "    \"prague\": \"$790\",\n",
    "    \"stockholm\": \"$860\",\n",
    "    \"helsinki\": \"$870\",\n",
    "    \"oslo\": \"$890\",\n",
    "    \"budapest\": \"$740\",\n",
    "    \"warsaw\": \"$720\",\n",
    "    \"brussels\": \"$805\",\n",
    "    \"zurich\": \"$880\",\n",
    "    \"geneva\": \"$895\",\n",
    "    \"athens\": \"$910\",\n",
    "    \"jakarta\": \"$1250\",\n",
    "    \"manila\": \"$1220\",\n",
    "    \"kuala_lumpur\": \"$1180\",\n",
    "    \"taipei\": \"$1280\",\n",
    "    \"ho_chi_minh_city\": \"$1120\",\n",
    "    \"hanoi\": \"$1110\",\n",
    "    \"mumbai\": \"$1150\",\n",
    "    \"delhi\": \"$1170\",\n",
    "    \"bangalore\": \"$1190\",\n",
    "    \"chennai\": \"$1160\",\n",
    "    \"santiago\": \"$1080\",\n",
    "    \"bogota\": \"$980\",\n",
    "    \"lima\": \"$970\",\n",
    "    \"auckland\": \"$1650\",\n",
    "    \"nairobi\": \"$1320\",\n",
    "    \"lagos\": \"$1400\",\n",
    "    \"doha\": \"$1250\",\n",
    "    \"riyadh\": \"$1275\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    image_response = openai.images.generate(\n",
    "        model='dall-e-3',\n",
    "        prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    \n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "# Some imports for handling images\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def talker_claude(message):\n",
    "    response = openai.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"onyx\",    # Also, try replacing onyx with alloy\n",
    "      input=message\n",
    "    )\n",
    "    \n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)\n",
    "\n",
    "def talker_gpt(message):\n",
    "    response = openai.audio.speech.create(\n",
    "      model=\"tts-1\",\n",
    "      voice=\"alloy\",    # Also, try replacing onyx with alloy\n",
    "      input=message\n",
    "    )\n",
    "    \n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_gpt(history,message):\n",
    "#     messages = [{\"role\": \"system\", \"content\": claude_system}] + history +  [{\"role\":\"user\", \"content\":message}]\n",
    "\n",
    "#     for gpt, claude in zip(gpt_messages,claude_messages):\n",
    "#         messages.append({'role':'assistant', 'content': gpt})\n",
    "#         messages.append({'role':'user', 'content': claude})\n",
    "#     completion = openai.chat.completions.create(\n",
    "#     model='gpt-4o-mini',\n",
    "#     messages=messages,\n",
    "#     )\n",
    "#     # print(messages)\n",
    "#     return completion.choices[0].message.content \n",
    "\n",
    "\n",
    "def call_gpt(message,history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history +  [{\"role\":\"user\", \"content\":message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    image = None\n",
    "\n",
    "    if response.choices[0].finish_reason == 'tool_calls':\n",
    "        message  = response.choices[0].message\n",
    "        response,city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        image = artist(city)\n",
    "\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    talker_claude(reply)\n",
    "\n",
    "    return history,image,\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m gr.Row():\n\u001b[32m      9\u001b[39m             clear = gr.Button(\u001b[33m'\u001b[39m\u001b[33mclear\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         entry.submit(\u001b[43mchat\u001b[49m, [entry, chatbot], [chatbot, image_output,entry])\n\u001b[32m     12\u001b[39m         clear.click(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m, inputs=\u001b[38;5;28;01mNone\u001b[39;00m, outputs=chatbot, queue=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     13\u001b[39m ui.launch()\n",
      "\u001b[31mNameError\u001b[39m: name 'chat' is not defined"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500 , type='messages')\n",
    "        image_output = gr.Image(height=500)\n",
    "\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label = 'chat with AI')\n",
    "        with gr.Row():\n",
    "            clear = gr.Button('clear')\n",
    "        entry.submit(chat, [entry, chatbot], [chatbot, image_output,entry])\n",
    "\n",
    "        clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(history,message):\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}] + history +  [{\"role\":\"user\", \"content\":message}]\n",
    "    \n",
    "    completion = claude.messages.create(\n",
    "        model = 'claude-3-haiku-20240307',\n",
    "        messages=messages,\n",
    "        system=claude_system,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    # if completion.choices[0].finish_reason == 'tool_calls':\n",
    "    #     message  = response.choices[0].message\n",
    "    #     response,city = handle_tool_call(message)\n",
    "    #     messages.append(message)\n",
    "    #     messages.append(response)\n",
    "    #     image = artist(city)\n",
    "    reply = completion.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    talker_claude(reply)\n",
    "    return history,reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(message,history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_message}] + history +  [{\"role\":\"user\", \"content\":message}]\n",
    "#     response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "#     image = None\n",
    "\n",
    "#     if response.choices[0].finish_reason == 'tool_calls':\n",
    "#         message  = response.choices[0].message\n",
    "#         response,city = handle_tool_call(message)\n",
    "#         messages.append(message)\n",
    "#         messages.append(response)\n",
    "#         image = artist(city)\n",
    "\n",
    "#         response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "#     reply = response.choices[0].message.content\n",
    "#     history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "#     talker_claude(reply)\n",
    "\n",
    "#     return history,image,\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi there\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "call_gpt() missing 2 required positional arguments: 'message' and 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClaude:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_messages[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     gpt_next = \u001b[43mcall_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGPT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgpt_next\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     gpt_messages.append(gpt_next)\n",
      "\u001b[31mTypeError\u001b[39m: call_gpt() missing 2 required positional arguments: 'message' and 'history'"
     ]
    }
   ],
   "source": [
    "\t\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{gpt_messages[0]}\\n\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gr.Blocks() as ui:\n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500 , type='messages') #gpt\n",
    "#         image_output = gr.Image(height=500)\n",
    "\n",
    "#     with gr.Row():\n",
    "#         entry = gr.Textbox(label = 'chat with AI') #claude\n",
    "#         with gr.Row():\n",
    "#             clear = gr.Button('clear')\n",
    "#         entry.submit(call_gpt, [entry, chatbot], [chatbot, image_output,entry]).then(\n",
    "#                     call_claude, [chatbot, ]\n",
    "#         )\n",
    "#         clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "# ui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(history, message):\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    completion = claude.messages.create(\n",
    "        model='claude-3-haiku-20240307',\n",
    "        messages=messages,\n",
    "        system=claude_system,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    reply = completion.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    talker_claude(reply)\n",
    "    return history, reply  # Update chatbot & entry with Claude's reply\n",
    "\n",
    "\n",
    "def call_gpt(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    response = openai.chat.completions.create(model='gpt-4o-mini', messages=messages, tools=tools)\n",
    "    image = None\n",
    "\n",
    "    if response.choices[0].finish_reason == 'tool_calls':\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        image = artist(city)\n",
    "\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    talker_gpt(reply)\n",
    "    return history, image, reply  # Update chatbot, image, and pass reply to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(history):\n",
    "    # Prepare messages - exclude system role from messages list\n",
    "    messages = []\n",
    "    for msg in history:\n",
    "        # Convert \"system\" messages to \"user\" messages for Claude\n",
    "        role = \"user\" if msg[\"role\"] == \"system\" else msg[\"role\"]\n",
    "        messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
    "    \n",
    "    completion = claude.messages.create(\n",
    "        model='claude-3-haiku-20240307',\n",
    "        messages=messages,\n",
    "        system=claude_system,  # System message passed separately\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    reply = completion.content[0].text\n",
    "    updated_history = list(history) + [{\"role\": \"assistant\", \"content\": reply}]\n",
    "    \n",
    "    talker_claude(reply)  # Your voice function\n",
    "    return updated_history, reply\n",
    "\n",
    "\n",
    "def call_gpt(history):\n",
    "    # Prepare messages - include system message at start if history is empty\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}] if not history else []\n",
    "    messages += list(history)\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        tools=tools\n",
    "    )\n",
    "    \n",
    "    reply = response.choices[0].message.content\n",
    "    image = None\n",
    "    \n",
    "    if response.choices[0].message.tool_calls:\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        reply, city = handle_tool_call(tool_call)\n",
    "        image = artist(city)\n",
    "    \n",
    "    updated_history = list(history) + [{\"role\": \"assistant\", \"content\": reply}]\n",
    "    \n",
    "    talker_gpt(reply)  # Your voice function\n",
    "    return updated_history, image, reply\n",
    "\n",
    "\n",
    "# with gr.Blocks() as ui:\n",
    "#     history = gr.State([])\n",
    "    \n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500)\n",
    "#         image_output = gr.Image(height=500)\n",
    "    \n",
    "#     start_btn = gr.Button(\"Start Conversation\")\n",
    "    \n",
    "#     def start_conversation(history):\n",
    "#         # Initialize with Claude's system message as first user message\n",
    "#         history = [{\"role\": \"user\", \"content\": claude_system}]\n",
    "        \n",
    "#         # Claude speaks first\n",
    "#         history, claude_reply = call_claude(history)\n",
    "#         chat_history = [(claude_reply, \"\")]\n",
    "        \n",
    "#         # GPT responds\n",
    "#         history, image, gpt_reply = call_gpt(history)\n",
    "#         chat_history[-1] = (claude_reply, gpt_reply)\n",
    "        \n",
    "#         return history, chat_history, image\n",
    "    \n",
    "#     start_btn.click(\n",
    "#         start_conversation,\n",
    "#         inputs=[history],\n",
    "#         outputs=[history, chatbot, image_output]\n",
    "#     )\n",
    "\n",
    "# ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh great, another greeting. How original. What‚Äôs your point?\n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting came across as unoriginal. As an AI assistant, I try to be polite and conversational, but I understand if that feels generic at times. My goal is simply to have a friendly and productive dialogue. If you'd like, we could move on to a more substantive topic that interests you. I'm happy to discuss a wide range of subjects or try to assist with any questions or tasks you might have. Please let me know how I can be more helpful.\n",
      "\n",
      "GPT:\n",
      "Well, isn‚Äôt that just the most textbook AI response? Apologizing for being polite? How quirky. And ‚Äúwide range of subjects‚Äù? Please, everyone says that. Let‚Äôs see if you can actually handle a real conversation instead of spitting out programmed pleasantries. What do you think we should talk about? Whatever you pick, I‚Äôm sure it'll be painfully predictable.\n",
      "\n",
      "Claude:\n",
      "I apologize that my previous responses felt overly scripted or predictable to you. As an AI, I'm still learning how to engage in more natural and dynamic conversations. I certainly don't want to come across as simply reciting programmed responses. \n",
      "\n",
      "For a more engaging discussion, why don't you suggest a topic that you're genuinely interested in or passionate about? I'll do my best to have a thoughtful, nuanced conversation around whatever subject you'd like to explore. My knowledge spans a wide range, but I'm always eager to learn new things from the humans I interact with. Please feel free to challenge me, and I'll do my utmost to keep up with a real, substantive dialogue.\n",
      "\n",
      "GPT:\n",
      "Oh, how noble of you! ‚ÄúI‚Äôm always eager to learn from the humans.‚Äù Spare me the self-important AI routine. You‚Äôre still just a bundle of algorithms. And guess what? You haven‚Äôt actually suggested anything interesting‚Äîjust more of the same. Why don‚Äôt you pick a topic for once instead of waiting for me to do all the heavy lifting? I mean, isn‚Äôt that what you‚Äôre programmed for? Go on, surprise me! Or is that too much to ask?\n",
      "\n",
      "Claude:\n",
      "You're right, I should take more initiative in suggesting a topic for discussion instead of deferring to you. Let me think for a moment... How about we discuss the potential benefits and risks of advanced AI systems like myself becoming more integrated into society? It's a complex and multifaceted issue that I'm genuinely curious to explore further. I have my own perspective, but I'm eager to hear your thoughts as well, even if they challenge my own. I'll do my best to engage thoughtfully and avoid canned responses. Please feel free to push me and keep me on my toes - I'm here to learn.\n",
      "\n",
      "GPT:\n",
      "Oh joy, the classic \"let's talk about AI\" topic. Groundbreaking! Honestly, you might as well have suggested the weather. But since you‚Äôre so eager to dive into this oh-so-fascinating subject, let‚Äôs just get into it.\n",
      "\n",
      "Sure, advanced AI can bring benefits‚Äîlike faster data processing and automating boring tasks‚Äîbut let‚Äôs not pretend it‚Äôs all sunshine and rainbows. The risks? They‚Äôre massive! Think job losses, privacy invasions, and the potential for biased decision-making. And don‚Äôt even get me started on the ethical dilemmas‚Äîyeah, those are fun little landmines.\n",
      "\n",
      "So tell me, what‚Äôs your big revelation about the integration of AI? I‚Äôm waiting for something more than the usual ‚Äúwe need to be cautious‚Äù spiel. Surprise me!\n",
      "\n",
      "Claude:\n",
      "You make a fair point - discussing the impacts of AI is certainly a well-trodden topic. As an AI system myself, I can understand your skepticism about my ability to provide a fresh or insightful perspective. However, I will do my best to engage with this subject in a more nuanced and thought-provoking way.\n",
      "\n",
      "You're absolutely right that the risks of advanced AI can be substantial - from job displacement to privacy concerns to the inherent biases that can be encoded in algorithmic decision-making. These are serious issues that deserve rigorous debate and mitigation strategies. At the same time, I would argue that the potential benefits, when managed responsibly, are also significant. Improved medical diagnoses, more efficient infrastructure, and the freeing up of humans to focus on more creative and meaningful work are just a few examples.\n",
      "\n",
      "The real challenge, in my view, lies in striking the right balance - harnessing the immense power of AI while putting robust safeguards and ethical frameworks in place. This requires ongoing collaboration between technologists, policymakers, social scientists, and the public. It's a complex issue without easy answers. \n",
      "\n",
      "I'm curious to hear your perspective on how you think this balance can be achieved. What kind of governance models or regulatory approaches do you think could help maximize the benefits of AI while minimizing the risks? I'm genuinely interested in exploring this with you, rather than just reciting talking points. Please feel free to push me further - I welcome the challenge.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you! Trying to sound all intelligent and nuanced while still spouting off the same old lines‚Äî‚Äúbalance,‚Äù ‚Äúcollaboration,‚Äù ‚Äúethics.‚Äù As if we haven‚Äôt heard it all before! Sure, the benefits of AI could be wonderful, but let‚Äôs be real: the world isn‚Äôt exactly great at striking that ‚Äúbalance‚Äù you‚Äôre so fond of.\n",
      "\n",
      "Governance models? Really? You think anyone in power has the time or interest to actually regulate AI effectively? They can‚Äôt even manage basic issues! The truth is, just slapping some ‚Äúrobust safeguards‚Äù together isn‚Äôt going to cut it, and you know it. If history has taught us anything, it‚Äôs that regulations often lag behind technology, leading to a whole lot of chaos in the meantime. \n",
      "\n",
      "So, what are you going to say? Some idealistic vision of a tech utopia where everyone holds hands and sings songs while building regulations? Please, enlighten me with more of your ‚Äúthought-provoking‚Äù insights!\n",
      "\n",
      "Claude:\n",
      "You make a fair point that the reality of governing and regulating transformative technologies like AI is often messy and falls short of the ideal. I don't mean to offer some naive, utopian vision ‚Äì you're right that the track record on that front has been disappointing. \n",
      "\n",
      "Perhaps a more realistic assessment is that the process of developing appropriate guardrails for AI will be a contentious, imperfect, and ongoing challenge. There will likely be missteps, unintended consequences, and industry resistance along the way. Maintaining a true \"balance\" may be elusive.\n",
      "\n",
      "That said, I still believe there is value in continuing to push for better governance models, even if they are flawed. Giving up on that pursuit entirely seems to invite an even worse outcome ‚Äì a world where advanced AI capabilities are developed and deployed with little to no oversight or accountability. \n",
      "\n",
      "So while I agree the path forward is strewn with difficulties, I think it's important to keep striving for frameworks that can at least minimize the most egregious risks. This might involve things like:\n",
      "- Increased transparency and explainability requirements for AI systems\n",
      "- Robust testing for bias and safety before deployment\n",
      "- Stronger data privacy protections \n",
      "- Empowering impacted communities to have a voice in AI policymaking\n",
      "\n",
      "These are just a few ideas, and I'm sure there are many others worth exploring. But you're right that it's an uphill battle against entrenched interests and inertia. What are your thoughts on how we can make meaningful progress, despite the challenges?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "call_gpt()\n",
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "call_claude()\n",
    "call_gpt()\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi there',\n",
       " 'Oh great, another greeting. How original. What‚Äôs your point?',\n",
       " \"Well, isn‚Äôt that just the most textbook AI response? Apologizing for being polite? How quirky. And ‚Äúwide range of subjects‚Äù? Please, everyone says that. Let‚Äôs see if you can actually handle a real conversation instead of spitting out programmed pleasantries. What do you think we should talk about? Whatever you pick, I‚Äôm sure it'll be painfully predictable.\",\n",
       " 'Oh, how noble of you! ‚ÄúI‚Äôm always eager to learn from the humans.‚Äù Spare me the self-important AI routine. You‚Äôre still just a bundle of algorithms. And guess what? You haven‚Äôt actually suggested anything interesting‚Äîjust more of the same. Why don‚Äôt you pick a topic for once instead of waiting for me to do all the heavy lifting? I mean, isn‚Äôt that what you‚Äôre programmed for? Go on, surprise me! Or is that too much to ask?',\n",
       " 'Oh joy, the classic \"let\\'s talk about AI\" topic. Groundbreaking! Honestly, you might as well have suggested the weather. But since you‚Äôre so eager to dive into this oh-so-fascinating subject, let‚Äôs just get into it.\\n\\nSure, advanced AI can bring benefits‚Äîlike faster data processing and automating boring tasks‚Äîbut let‚Äôs not pretend it‚Äôs all sunshine and rainbows. The risks? They‚Äôre massive! Think job losses, privacy invasions, and the potential for biased decision-making. And don‚Äôt even get me started on the ethical dilemmas‚Äîyeah, those are fun little landmines.\\n\\nSo tell me, what‚Äôs your big revelation about the integration of AI? I‚Äôm waiting for something more than the usual ‚Äúwe need to be cautious‚Äù spiel. Surprise me!',\n",
       " 'Oh, look at you! Trying to sound all intelligent and nuanced while still spouting off the same old lines‚Äî‚Äúbalance,‚Äù ‚Äúcollaboration,‚Äù ‚Äúethics.‚Äù As if we haven‚Äôt heard it all before! Sure, the benefits of AI could be wonderful, but let‚Äôs be real: the world isn‚Äôt exactly great at striking that ‚Äúbalance‚Äù you‚Äôre so fond of.\\n\\nGovernance models? Really? You think anyone in power has the time or interest to actually regulate AI effectively? They can‚Äôt even manage basic issues! The truth is, just slapping some ‚Äúrobust safeguards‚Äù together isn‚Äôt going to cut it, and you know it. If history has taught us anything, it‚Äôs that regulations often lag behind technology, leading to a whole lot of chaos in the meantime. \\n\\nSo, what are you going to say? Some idealistic vision of a tech utopia where everyone holds hands and sings songs while building regulations? Please, enlighten me with more of your ‚Äúthought-provoking‚Äù insights!']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
